{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\skuma\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "line= [\n",
    "    'I am a student at theem college', \n",
    "    'i was a student at holyspirit school',\n",
    "    'i was also a student at sdsm college and was best!',\n",
    "    'i was a student at blossom school when i was little'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = Tokenizer(num_words = 100) # num_words=100 means assume we have dataset of many books in that case there will be thosands of token for each word to get most 100 repeated word we use num_words.\n",
    "token.fit_on_texts(line)\n",
    "Word_index= token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 1, 'was': 2, 'a': 3, 'student': 4, 'at': 5, 'college': 6, 'school': 7, 'am': 8, 'theem': 9, 'holyspirit': 10, 'also': 11, 'sdsm': 12, 'and': 13, 'best': 14, 'blossom': 15, 'when': 16, 'little': 17}\n"
     ]
    }
   ],
   "source": [
    "print(Word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences=token.texts_to_sequences(line) # all sentences in 'line' will converted into sequences based on token given to a word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 8, 3, 4, 5, 9, 6], [1, 2, 3, 4, 5, 10, 7], [1, 2, 11, 3, 4, 5, 12, 6, 13, 2, 14], [1, 2, 3, 4, 5, 15, 7, 16, 1, 2, 17]]\n"
     ]
    }
   ],
   "source": [
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4, 5], [3, 14]]\n"
     ]
    }
   ],
   "source": [
    "# what will happen if we try to fit this sequences on sentences which has no token in 'token list' ?, so let see it\n",
    "test=[\n",
    "    'i was not a student at sundaram',\n",
    "    'sundaram is a best'\n",
    "      ]\n",
    "test_seq=token.texts_to_sequences(test)\n",
    "print(test_seq)\n",
    "# it does give error but it not show any token for it in sequences because there is no token for it in token list \n",
    "# to solve this we can use oov_token = \"<oov>\" as it give common token to every unknown word, it is not effective way as data is lost but atleast it point the unknown word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<oov>': 1, 'i': 2, 'was': 3, 'a': 4, 'student': 5, 'at': 6, 'college': 7, 'school': 8, 'am': 9, 'theem': 10, 'holyspirit': 11, 'also': 12, 'sdsm': 13, 'and': 14, 'best': 15, 'blossom': 16, 'when': 17, 'little': 18}\n"
     ]
    }
   ],
   "source": [
    "token = Tokenizer(num_words = 100,oov_token=\"<oov>\") # num_words=100 means assume we have dataset of many books in that case there will be thosands of token for each word to get most 100 repeated word we use num_words.\n",
    "token.fit_on_texts(line)\n",
    "Word_index= token.word_index\n",
    "print(Word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3, 1, 4, 5, 6, 1], [1, 1, 4, 15]]\n"
     ]
    }
   ],
   "source": [
    "test=[\n",
    "    'i was not a student at sundaram',\n",
    "    'sundaram is a best'\n",
    "      ]\n",
    "test_seq=token.texts_to_sequences(test)\n",
    "print(test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  8  3  4  5  9  6  0  0  0  0  0  0  0  0]\n",
      " [ 1  2  3  4  5 10  7  0  0  0  0  0  0  0  0]\n",
      " [ 1  2 11  3  4  5 12  6 13  2 14  0  0  0  0]\n",
      " [ 1  2  3  4  5 15  7 16  1  2 17  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "#  every sentences as different length and because of it we also get diffrent size of sequences, to make it equal we add 0s in it, and to do that we use padding\n",
    "# padding='post' means add 0s at last places\n",
    "# maxlen=5 means maximum lenght of sequencs\n",
    "# truncating='pre' means cut the sequences from starting place\n",
    "\n",
    "padd=pad_sequences(sequences,padding='post',maxlen=15,truncating='pre') \n",
    "print(padd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
